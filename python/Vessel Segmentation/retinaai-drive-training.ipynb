{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968b1dfe",
   "metadata": {
    "papermill": {
     "duration": 0.005434,
     "end_time": "2025-07-10T05:36:57.498926",
     "exception": false,
     "start_time": "2025-07-10T05:36:57.493492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Retinal Fundus Dataset Visualization\n",
    "\n",
    "This notebook visualizes the DRIVE, HRF, CHASEDB1, and STARE datasets for retinal vessel segmentation. It includes dataset descriptions, folder hierarchy, image counts, formats, resolutions, and visualizations of sample images and data distributions.\n",
    "\n",
    "**You can get a quick overview of major dataset on this link too** [Datasets Overview](https://www.medicmind.tech/retinal-image-databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4aecee",
   "metadata": {
    "papermill": {
     "duration": 0.004006,
     "end_time": "2025-07-10T05:36:57.507393",
     "exception": false,
     "start_time": "2025-07-10T05:36:57.503387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Dataset Folder Hierarchy\n",
    "\n",
    "The datasets are stored in the `/kaggle/input/final_dataset/` directory with the following structure (corrected for the typo in DRIVE training masks):\n",
    "\n",
    "```\n",
    "final_dataset/\n",
    "├── CHASEDB1/\n",
    "│   ├── images/\n",
    "│   │   ├── Image_01L.jpg\n",
    "│   │   ├── Image_01R.jpg\n",
    "│   │   └── ... (28 images)\n",
    "│   ├── labels1/\n",
    "│   │   ├── Image_01L_1stHO.png\n",
    "│   │   ├── Image_01R_1stHO.png\n",
    "│   │   └── ... (28 images)\n",
    "│   ├── labels2/\n",
    "│   │   ├── Image_01L_2ndHO.png\n",
    "│   │   ├── Image_01R_2ndHO.png\n",
    "│   │   └── ... (28 images)\n",
    "│   ├── masks/\n",
    "│   │   ├── mask_01L.png\n",
    "│   │   ├── mask_01R.png\n",
    "│   │   └── ... (28 images)\n",
    "├── DRIVE/\n",
    "│   ├── test/\n",
    "│   │   ├── 1st_manual/\n",
    "│   │   │   ├── 01_manual1.gif\n",
    "│   │   │   ├── 02_manual1.gif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   │   ├── 2nd_manual/\n",
    "│   │   │   ├── 01_manual2.gif\n",
    "│   │   │   ├── 02_manual2.gif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   │   ├── images/\n",
    "│   │   │   ├── 01_test.tif\n",
    "│   │   │   ├── 02_test.tif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   │   ├── mask/\n",
    "│   │   │   ├── 01_test_mask.gif\n",
    "│   │   │   ├── 02_test_mask.gif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   ├── training/\n",
    "│   │   ├── 1st_manual/\n",
    "│   │   │   ├── 21_manual1.gif\n",
    "│   │   │   ├── 22_manual1.gif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   │   ├── images/\n",
    "│   │   │   ├── 21_training.tif\n",
    "│   │   │   ├── 22_training.tif\n",
    "│   │   │   └── ... (20 images)\n",
    "│   │   ├── mask/\n",
    "│   │   │   ├── 21_training_mask.gif\n",
    "│   │   │   ├── 22_training_mask.gif\n",
    "│   │   │   └── ... (20 images)\n",
    "├── HRF/\n",
    "│   ├── images/\n",
    "│   │   ├── 01_dr.JPG\n",
    "│   │   ├── 01_g.jpg\n",
    "│   │   ├── 01_h.jpg\n",
    "│   │   └── ... (45 images)\n",
    "│   ├── manual1/\n",
    "│   │   ├── 01_dr.tif\n",
    "│   │   │   ├── 01_g.tif\n",
    "│   │   │   ├── 01_h.tif\n",
    "│   │   │   └── ... (45 images)\n",
    "│   ├── mask/\n",
    "│   │   ├── 01_dr_mask.tif\n",
    "│   │   │   ├── 01_g_mask.tif\n",
    "│   │   │   ├── 01_h_mask.tif\n",
    "│   │   │   └── ... (45 images)\n",
    "├── STARE/\n",
    "│   ├── images/\n",
    "│   │   ├── im0001.ppm\n",
    "│   │   ├── im0002.ppm\n",
    "│   │   └── ... (20 images, non-sequential numbering)\n",
    "│   ├── labels-ah/\n",
    "│   │   ├── im0001.ah.ppm\n",
    "│   │   ├── im0002.ah.ppm\n",
    "│   │   └── ... (20 images, non-sequential numbering)\n",
    "│   ├── masks/\n",
    "│   │   ├── mask_0001.png\n",
    "│   │   ├── mask_0002.png\n",
    "│   │   └── ... (20 images, non-sequential numbering)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b815a52",
   "metadata": {
    "papermill": {
     "duration": 0.004072,
     "end_time": "2025-07-10T05:36:57.542958",
     "exception": false,
     "start_time": "2025-07-10T05:36:57.538886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: CHASEDB1 Dataset Overview\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Online/Release**: CHASEDB1 (Child Heart and Health Study in England Database) is a publicly available dataset for retinal vessel segmentation, containing 28 fundus images of left and right eyes from 14 children. Originally, masks were not included but were retrieved from [GitHub](https://github.com/zhengyuan-liu/Retinal-Vessel-Segmentation). Original link: [CHASEDB1](https://www.kaggle.com/datasets/khoongweihao/chasedb1/data)\n",
    "- **In Code**: Stored in `/kaggle/input/final_dataset/CHASEDB1/` with subfolders: `images` (.jpg), `labels1` (.png, first annotator), `labels2` (.png, second annotator), and `masks` (.png).\n",
    "- **Count**: 28 images, 28 masks, 28 first annotations, 28 second annotations.\n",
    "- **Format**: Images (.jpg), Masks/Labels (.png).\n",
    "- **Resolution**: Typically 999x960 pixels.\n",
    "- **Division**: No predefined train/test split.\n",
    "\n",
    "**Output**: Displays counts, unique resolutions, and a row of sample images (original, mask, first annotation, second annotation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6efdf5",
   "metadata": {
    "papermill": {
     "duration": 0.009858,
     "end_time": "2025-07-10T05:36:58.947724",
     "exception": false,
     "start_time": "2025-07-10T05:36:58.937866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: DRIVE Dataset Overview\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Online/Release**: DRIVE (Digital Retinal Images for Vessel Extraction) contains 40 fundus images, split into 20 training and 20 test images. Test set manual annotations were originally missing but retrieved from [GitHub](https://github.com/zhengyuan-liu/Retinal-Vessel-Segmentation). Original link: [DRIVE](https://www.kaggle.com/datasets/andrewmvd/drive-digital-retinal-images-for-vessel-extraction)\n",
    "- **In Code**: Stored in `/kaggle/input/final_dataset/DRIVE/` with `test` and `training` subfolders, each containing `images` (.tif), `1st_manual` (.gif), `2nd_manual` (.gif, test only), and `mask` (.gif).\n",
    "- **Count**: Training: 20 images, 20 masks, 20 first annotations. Test: 20 images, 20 masks, 20 first annotations, 20 second annotations.\n",
    "- **Format**: Images (.tif), Masks/Annotations (.gif).\n",
    "- **Resolution**: 565x584 pixels.\n",
    "- **Division**: Predefined train/test split (20/20).\n",
    "\n",
    "**Output**: Displays counts, resolutions, and two rows of sample images (training and test sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad010d94",
   "metadata": {
    "papermill": {
     "duration": 0.019392,
     "end_time": "2025-07-10T05:37:01.415817",
     "exception": false,
     "start_time": "2025-07-10T05:37:01.396425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: HRF Dataset Overview\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Online/Release**: HRF (High-Resolution Fundus) contains 45 high-resolution fundus images: 15 healthy (h), 15 diabetic retinopathy (dr), 15 glaucomatous (g). Available publicly with masks and annotations. Original link: [HRF](https://datasetninja.com/high-resolution-fundus\n",
    ")\n",
    "- **In Code**: Stored in `/kaggle/input/final_dataset/HRF/` with `images` (.JPG), `manual1` (.tif), and `mask` (.tif).\n",
    "- **Count**: 45 images, 45 masks, 45 annotations.\n",
    "- **Format**: Images (.JPG), Masks/Annotations (.tif).\n",
    "- **Resolution**: Typically 3504x2336 pixels.\n",
    "- **Division**: No predefined train/test split.\n",
    "\n",
    "**Output**: Displays counts, resolutions, sample images for each category, and a bar plot of category distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36871488",
   "metadata": {
    "papermill": {
     "duration": 0.035354,
     "end_time": "2025-07-10T05:37:10.644503",
     "exception": false,
     "start_time": "2025-07-10T05:37:10.609149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: STARE Dataset Overview\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Online/Release**: STARE (Structured Analysis of the Retina) contains 20 fundus images with non-sequential numbering. Masks were not originally included but retrieved from [GitHub](https://github.com/zhengyuan-liu/Retinal-Vessel-Segmentation). Original Link: [Official STARE](https://cecas.clemson.edu/~ahoover/stare/) , [Kaggle STARE](https://www.kaggle.com/datasets/vidheeshnacode/stare-dataset)\n",
    "- **In Code**: Stored in `/kaggle/input/final_dataset/STARE/` with `images` (.ppm), `labels-ah` (.ppm), and `masks` (.png). File names handled dynamically due to irregular numbering.\n",
    "- **Count**: 20 images, 20 masks, 20 annotations.\n",
    "- **Format**: Images/Annotations (.ppm), Masks (.png).\n",
    "- **Resolution**: Typically 700x605 pixels.\n",
    "- **Division**: No predefined train/test split.\n",
    "\n",
    "**Output**: Displays counts, resolutions, and sample images, dynamically handling file names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80930b23",
   "metadata": {
    "papermill": {
     "duration": 0.039205,
     "end_time": "2025-07-10T05:37:12.040033",
     "exception": false,
     "start_time": "2025-07-10T05:37:12.000828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model For Training: R2U-Net for Retinal Blood Vessel Segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8eda8f",
   "metadata": {
    "papermill": {
     "duration": 0.038084,
     "end_time": "2025-07-10T05:37:12.196462",
     "exception": false,
     "start_time": "2025-07-10T05:37:12.158378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We import necessary libraries for data handling, image processing, model building, and evaluation. TensorFlow/Keras is used for the R2U-Net implementation, and Matplotlib for visualization.\n",
    "\n",
    "**Output**: Libraries are loaded, and the base path is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1b9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T05:37:12.273982Z",
     "iopub.status.busy": "2025-07-10T05:37:12.273716Z",
     "iopub.status.idle": "2025-07-10T05:37:27.698967Z",
     "shell.execute_reply": "2025-07-10T05:37:27.698143Z"
    },
    "papermill": {
     "duration": 15.465999,
     "end_time": "2025-07-10T05:37:27.700473",
     "exception": false,
     "start_time": "2025-07-10T05:37:12.234474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "import random\n",
    "import cv2\n",
    "import logging\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Base path for dataset\n",
    "base_path = '/kaggle/input/final-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd3e73",
   "metadata": {
    "papermill": {
     "duration": 0.036487,
     "end_time": "2025-07-10T05:37:27.774718",
     "exception": false,
     "start_time": "2025-07-10T05:37:27.738231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Define Helper Functions\n",
    "\n",
    "We define functions for loading images, extracting patches, and computing evaluation metrics (Accuracy, Sensitivity, Specificity, F1-Score, Dice Coefficient, Jaccard Similarity, AUC). These are used across all datasets.\n",
    "\n",
    "**Output**: Helper functions are defined for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568763e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T05:37:27.850935Z",
     "iopub.status.busy": "2025-07-10T05:37:27.849902Z",
     "iopub.status.idle": "2025-07-10T05:37:27.869342Z",
     "shell.execute_reply": "2025-07-10T05:37:27.868761Z"
    },
    "papermill": {
     "duration": 0.058683,
     "end_time": "2025-07-10T05:37:27.870308",
     "exception": false,
     "start_time": "2025-07-10T05:37:27.811625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load and preprocess retinal fundus image to enhance vessel visibility with controlled contrast.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if not already (in case of grayscale or other formats)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Extract green channel (vessels are most prominent in green channel)\n",
    "    img_array = np.array(img)\n",
    "    green_channel = img_array[:, :, 1]  # Green channel (index 1 in RGB)\n",
    "    \n",
    "    # Apply CLAHE to enhance contrast with reduced amplification\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    green_channel = clahe.apply(green_channel)\n",
    "    \n",
    "    return green_channel\n",
    "\n",
    "def extract_patches(image, mask, patch_size=48, num_patches=10000):\n",
    "    \"\"\"Extract random patches from image and mask.\"\"\"\n",
    "    patches = []\n",
    "    labels = []\n",
    "    h, w = image.shape\n",
    "    for _ in range(num_patches):\n",
    "        y = random.randint(0, h - patch_size)\n",
    "        x = random.randint(0, w - patch_size)\n",
    "        patch = image[y:y+patch_size, x:x+patch_size]\n",
    "        label = mask[y:y+patch_size, x:x+patch_size]\n",
    "        patches.append(patch[..., np.newaxis])  # Add channel dimension\n",
    "        labels.append(label[..., np.newaxis] / 255.0)  # Normalize to [0,1]\n",
    "    return np.array(patches), np.array(labels)\n",
    "\n",
    "def extract_patches_for_eval(image, patch_size=48):\n",
    "    \"\"\"Extract all possible 48x48 non-overlapping patches from an image for evaluation.\"\"\"\n",
    "    patches = []\n",
    "    h, w = image.shape[:2]\n",
    "    for y in range(0, h - patch_size + 1, patch_size):  # Non-overlapping patches\n",
    "        for x in range(0, w - patch_size + 1, patch_size):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(patch[..., np.newaxis])  # Add channel dimension\n",
    "    return np.array(patches), h, w\n",
    "\n",
    "def reconstruct_image(patches, h, w, patch_size=48):\n",
    "    \"\"\"Reconstruct image from non-overlapping patches.\"\"\"\n",
    "    recon = np.zeros((h, w, 1))\n",
    "    patch_idx = 0\n",
    "    for y in range(0, h - patch_size + 1, patch_size):\n",
    "        for x in range(0, w - patch_size + 1, patch_size):\n",
    "            if patch_idx < len(patches):  # Ensure index is valid\n",
    "                recon[y:y+patch_size, x:x+patch_size] = patches[patch_idx]\n",
    "            patch_idx += 1\n",
    "    return recon\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute evaluation metrics: AC, SE, SP, F1, DC, JS, AUC.\"\"\"\n",
    "    y_true = (y_true.flatten() > 0.5).astype(np.int32)  # Convert float64 to int32\n",
    "    y_pred_binary = (y_pred.flatten() > 0.5).astype(np.int32)  # Threshold at 0.5\n",
    "    # Replace NaN or inf with 0 to avoid issues in metrics computation\n",
    "    y_pred_binary = np.nan_to_num(y_pred_binary, nan=0, posinf=0, neginf=0)\n",
    "    confusion = y_true * 2 + y_pred_binary\n",
    "    tn, fp, fn, tp = np.bincount(confusion, minlength=4)[0:4]\n",
    "    ac = (tp + tn) / (tp + tn + fp + fn + 1e-10)\n",
    "    se = tp / (tp + fn + 1e-10)\n",
    "    sp = tn / (tn + fp + 1e-10)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_binary, average='binary', zero_division=0)\n",
    "    dc = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    js = precision * recall / (precision + recall - precision * recall + 1e-10)\n",
    "    # Handle NaN in y_pred before AUC calculation\n",
    "    y_pred_flat = np.nan_to_num(y_pred.flatten(), nan=0)\n",
    "    auc = roc_auc_score(y_true, y_pred_flat)\n",
    "    return {'AC': ac, 'SE': se, 'SP': sp, 'F1': f1, 'DC': dc, 'JS': js, 'AUC': auc}\n",
    "\n",
    "def plot_sample_patches(patches, labels, preds, num_samples=5):\n",
    "    \"\"\"Plot sample patches, ground truth, and predictions.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(patches[i, :, :, 0], cmap='gray')\n",
    "        axes[i, 0].set_title('Input Patch')\n",
    "        axes[i, 1].imshow(labels[i, :, :, 0], cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 2].imshow(preds[i, :, :, 0] > 0.5, cmap='gray')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        for ax in axes[i]: ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "##########   New functions added for better training process (i.e preventing overfitting)   ############\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Compute Dice loss for binary segmentation.\"\"\"\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"Combine binary cross-entropy and Dice loss.\"\"\"\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return 0.5 * bce + 0.5 * dice\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Compute Dice coefficient for evaluation during training.\"\"\"\n",
    "    y_true_f = tf.keras.backend.flatten(tf.cast(y_true, tf.float32))  # Cast to float32\n",
    "    y_pred_f = tf.keras.backend.flatten(tf.cast(tf.where(y_pred > 0.5, 1.0, 0.0), tf.float32))  # Cast to float32\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "    \n",
    "class DiceCoefficientCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to compute Dice Coefficient and save best model based on val_dice_coefficient.\"\"\"\n",
    "    def __init__(self, checkpoint_path, val_patches, val_labels, monitor='val_dice_coefficient', mode='max'):\n",
    "        super(DiceCoefficientCallback, self).__init__()\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.val_patches = val_patches\n",
    "        self.val_labels = val_labels\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Compute Dice Coefficient for validation\n",
    "        val_pred = self.model.predict(self.val_patches, verbose=0)\n",
    "        current_val_dice = dice_coefficient(self.val_labels, val_pred)\n",
    "        logs['val_dice_coefficient'] = float(current_val_dice)\n",
    "        \n",
    "        # Check if current epoch is the best based on monitor\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if (self.mode == 'max' and current > self.best) or (self.mode == 'min' and current < self.best):\n",
    "            self.best = current\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.model.save(self.checkpoint_path)\n",
    "            print(f\"\\nEpoch {epoch + 1}: saving model to {self.checkpoint_path} with {self.monitor}={current:.4f}\")\n",
    "        \n",
    "        # Log metrics\n",
    "        logs['val_dice_coefficient'] = float(current_val_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf52203",
   "metadata": {
    "papermill": {
     "duration": 0.036129,
     "end_time": "2025-07-10T05:37:27.943532",
     "exception": false,
     "start_time": "2025-07-10T05:37:27.907403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Load and Preprocess Datasets\n",
    "\n",
    "We load DRIVE, STARE, CHASE_DB1, and HRF datasets, applying the paper’s preprocessing (cropping DRIVE to 565x565, using provided masks for STARE/CHASE_DB1, patch-based for HRF). We extract 190,000 patches for DRIVE and 250,000 for STARE, CHASE_DB1, and HRF, following the paper’s splits.\n",
    "\n",
    "**Output**: Prints the shapes of training/validation patches and the number of test images for each dataset. Expected shapes: DRIVE (171,000/19,000 patches, 20 test images), STARE/CHASE_DB1/HRF (225,000/25,000 patches, 4/8/9 test images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf07f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T05:37:28.018062Z",
     "iopub.status.busy": "2025-07-10T05:37:28.017797Z",
     "iopub.status.idle": "2025-07-10T05:37:55.144047Z",
     "shell.execute_reply": "2025-07-10T05:37:55.143131Z"
    },
    "papermill": {
     "duration": 27.165133,
     "end_time": "2025-07-10T05:37:55.145232",
     "exception": false,
     "start_time": "2025-07-10T05:37:27.980099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, total_patches, train_patches, val_patches):\n",
    "    \"\"\"Load images and extract patches for a dataset with predefined splits.\"\"\"\n",
    "    dataset_path = os.path.join(base_path, dataset_name)\n",
    "    \n",
    "    if dataset_name == 'DRIVE':\n",
    "        train_image_path = os.path.join(dataset_path, 'training/images')\n",
    "        train_mask_path = os.path.join(dataset_path, 'training/mask')\n",
    "        train_label_path = os.path.join(dataset_path, 'training/1st_manual')\n",
    "        test_image_path = os.path.join(dataset_path, 'test/images')\n",
    "        test_mask_path = os.path.join(dataset_path, 'test/mask')\n",
    "        test_label_path = os.path.join(dataset_path, 'test/1st_manual')\n",
    "        \n",
    "        train_image_files = sorted([f for f in os.listdir(train_image_path) if f.endswith(('.tif', '.TIF'))])\n",
    "        train_mask_files = sorted([f for f in os.listdir(train_mask_path) if f.endswith(('.gif', '.GIF'))])\n",
    "        train_label_files = sorted([f for f in os.listdir(train_label_path) if f.endswith(('.gif', '.GIF'))])\n",
    "        test_image_files = sorted([f for f in os.listdir(test_image_path) if f.endswith(('.tif', '.TIF'))])\n",
    "        test_mask_files = sorted([f for f in os.listdir(test_mask_path) if f.endswith(('.gif', '.GIF'))])\n",
    "        test_label_files = sorted([f for f in os.listdir(test_label_path) if f.endswith(('.gif', '.GIF'))])\n",
    "        \n",
    "        train_files = train_image_files\n",
    "        test_files = test_image_files\n",
    "    else:\n",
    "        image_path = os.path.join(dataset_path, 'images')\n",
    "        mask_path = os.path.join(dataset_path, 'masks' if dataset_name in ['STARE', 'CHASEDB1'] else 'mask')\n",
    "        label_path = os.path.join(dataset_path, 'labels-ah' if dataset_name == 'STARE' else 'labels1' if dataset_name == 'CHASEDB1' else 'manual1')\n",
    "        \n",
    "        image_files = sorted([f for f in os.listdir(image_path) if f.endswith(('.jpg', '.JPG', '.ppm'))])\n",
    "        mask_files = sorted([f for f in os.listdir(mask_path) if f.endswith(('.png', '.PNG', '.tif', '.TIF'))])\n",
    "        label_files = sorted([f for f in os.listdir(label_path) if f.endswith(('.ppm', '.PPM', '.png', '.PNG', '.tif', '.TIF'))])\n",
    "        \n",
    "        if dataset_name == 'CHASEDB1':\n",
    "            train_files = image_files[:20]\n",
    "            test_files = image_files[20:]\n",
    "        elif dataset_name == 'HRF':\n",
    "            train_files = image_files[:36]  # 80% of 45 images\n",
    "            test_files = image_files[36:]\n",
    "        else:  # STARE\n",
    "            train_files = image_files[:16]  # ~80% of 20 images\n",
    "            test_files = image_files[16:]\n",
    "    \n",
    "    train_patches_list, train_labels_list = [], []\n",
    "    val_patches_list, val_labels_list = [], []\n",
    "    test_images, test_labels = [], []\n",
    "\n",
    "    # Training and validation patches\n",
    "    for i, img_file in enumerate(train_files):\n",
    "        if dataset_name == 'DRIVE':\n",
    "            img = load_image(os.path.join(train_image_path, img_file))\n",
    "            label = load_image(os.path.join(train_label_path, train_label_files[i]))\n",
    "        else:\n",
    "            img = load_image(os.path.join(image_path, img_file))\n",
    "            label = load_image(os.path.join(label_path, label_files[i]))\n",
    "        \n",
    "        if dataset_name == 'DRIVE':\n",
    "            img = img[:, 9:574]  # Crop to 565x565\n",
    "            label = label[:, 9:574]\n",
    "        \n",
    "        # Adjust patch numbers based on memory constraints\n",
    "        patches_per_image = train_patches // len(train_files)\n",
    "        if dataset_name == 'HRF':\n",
    "            patches_per_image = 1250  # Reduced from ~6250 (225000/36) to fit memory\n",
    "        patches, labels = extract_patches(img, label, num_patches=patches_per_image)\n",
    "        split_idx = int(0.9 * len(patches))\n",
    "        train_patches_list.append(patches[:split_idx])\n",
    "        train_labels_list.append(labels[:split_idx])\n",
    "        val_patches_list.append(patches[split_idx:])\n",
    "        val_labels_list.append(labels[split_idx:])\n",
    "\n",
    "    \n",
    "    train_patches = np.concatenate(train_patches_list, axis=0)\n",
    "    train_labels = np.concatenate(train_labels_list, axis=0)\n",
    "    val_patches = np.concatenate(val_patches_list, axis=0)\n",
    "    val_labels = np.concatenate(val_labels_list, axis=0)\n",
    "\n",
    "    # Test images (full images for evaluation)\n",
    "    for i, img_file in enumerate(test_files):\n",
    "        if dataset_name == 'DRIVE':\n",
    "            img = load_image(os.path.join(test_image_path, img_file))\n",
    "            label = load_image(os.path.join(test_label_path, test_label_files[i]))\n",
    "        else:\n",
    "            img = load_image(os.path.join(image_path, img_file))\n",
    "            label = load_image(os.path.join(label_path, label_files[i + len(train_files)]))\n",
    "        \n",
    "        if dataset_name == 'DRIVE':\n",
    "            img = img[:, 9:574]\n",
    "            label = label[:, 9:574]\n",
    "        \n",
    "        test_images.append(img[..., np.newaxis])\n",
    "        test_labels.append(label[..., np.newaxis] / 255.0)\n",
    "    \n",
    "    return train_patches, train_labels, val_patches, val_labels, test_images, test_labels\n",
    "\n",
    "# Load datasets with original patch numbers and splits\n",
    "\n",
    "# original from paper\n",
    "# ('DRIVE', 190000, 171000, 19000)\n",
    "# ('STARE', 250000, 225000, 25000)\n",
    "# ('CHASEDB1', 250000, 225000, 25000)\n",
    "# ('HRF', 250000, 225000, 25000) <- This will cause memory issue and will break code, dont use this\n",
    "\n",
    "# subset for testing\n",
    "drive_patches_train, drive_labels_train, drive_patches_val, drive_labels_val, drive_test_images, drive_test_labels = load_dataset('DRIVE', 190000, 171000, 19000)\n",
    "stare_patches_train, stare_labels_train, stare_patches_val, stare_labels_val, stare_test_images, stare_test_labels = load_dataset('STARE', 250000, 225000, 25000)\n",
    "chase_patches_train, chase_labels_train, chase_patches_val, chase_labels_val, chase_test_images, chase_test_labels = load_dataset('CHASEDB1', 250000, 225000, 25000)\n",
    "hrf_patches_train, hrf_labels_train, hrf_patches_val, hrf_labels_val, hrf_test_images, hrf_test_labels = load_dataset('HRF', 45000, 40500, 4500)  # Reduced to 45,000 total\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"DRIVE: Train patches {drive_patches_train.shape}, Val patches {drive_patches_val.shape}, Test images {len(drive_test_images)}\")\n",
    "print(f\"STARE: Train patches {stare_patches_train.shape}, Val patches {stare_patches_val.shape}, Test images {len(stare_test_images)}\")\n",
    "print(f\"CHASEDB1: Train patches {chase_patches_train.shape}, Val patches {chase_patches_val.shape}, Test images {len(chase_test_images)}\")\n",
    "print(f\"HRF: Train patches {hrf_patches_train.shape}, Val patches {hrf_patches_val.shape}, Test images {len(hrf_test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc4a59",
   "metadata": {
    "papermill": {
     "duration": 0.036428,
     "end_time": "2025-07-10T05:37:55.220022",
     "exception": false,
     "start_time": "2025-07-10T05:37:55.183594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Define R2U-Net Model (( t=3 ))\n",
    "\n",
    "We implement the R2U-Net model with ( t=3 ) (1 forward convolution + 3 recurrent convolutions) using the architecture ( 1 -> 16 -> 32 -> 64 -> 128 -> 64 -> 32 -> 16 -> 1 ). The model includes recurrent residual convolutional units (RRCU) and skip connections with concatenation.\n",
    "\n",
    "**Output**: Displays the model summary, showing ~1.037M parameters for the ( t=3 ) architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99d109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T05:37:55.294856Z",
     "iopub.status.busy": "2025-07-10T05:37:55.294324Z",
     "iopub.status.idle": "2025-07-10T05:37:55.302176Z",
     "shell.execute_reply": "2025-07-10T05:37:55.301652Z"
    },
    "papermill": {
     "duration": 0.046288,
     "end_time": "2025-07-10T05:37:55.303221",
     "exception": false,
     "start_time": "2025-07-10T05:37:55.256933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rrcu_block(inputs, filters, kernel_size=3, t=3):\n",
    "    \"\"\"Recurrent Residual Convolutional Unit (RRCU) with t time steps.\"\"\"\n",
    "    conv = layers.Conv2D(filters, kernel_size, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(conv)\n",
    "    x = layers.ReLU()(x)\n",
    "    for _ in range(t-1):  # t-1 recurrent convolutions\n",
    "        conv_r = layers.Conv2D(filters, kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "        x = layers.Add()([conv, conv_r])  # Residual connection\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Add dropout for regularization\n",
    "    return x\n",
    "\n",
    "def r2unet(input_shape=(48, 48, 1)):\n",
    "    \"\"\"R2U-Net model with t=3.\"\"\"\n",
    "    inputs = layers.Input(input_shape)\n",
    "    # Encoder\n",
    "    e1 = rrcu_block(inputs, 16)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(e1)\n",
    "    e2 = rrcu_block(p1, 32)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(e2)\n",
    "    e3 = rrcu_block(p2, 64)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(e3)\n",
    "    e4 = rrcu_block(p3, 128)\n",
    "    # Decoder\n",
    "    u3 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(e4)\n",
    "    u3 = layers.Concatenate()([u3, e3])\n",
    "    d3 = rrcu_block(u3, 64)\n",
    "    u2 = layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(d3)\n",
    "    u2 = layers.Concatenate()([u2, e2])\n",
    "    d2 = rrcu_block(u2, 32)\n",
    "    u1 = layers.Conv2DTranspose(16, 2, strides=(2, 2), padding='same')(d2)\n",
    "    u1 = layers.Concatenate()([u1, e1])\n",
    "    d1 = rrcu_block(u1, 16)\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(d1)\n",
    "    return tf.keras.models.Model(inputs, outputs)  # Explicitly use tf.keras.models.Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea23b3",
   "metadata": {
    "papermill": {
     "duration": 0.039366,
     "end_time": "2025-07-10T05:37:55.380996",
     "exception": false,
     "start_time": "2025-07-10T05:37:55.341630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Train R2U-Net on Each Dataset\n",
    "\n",
    "We train the R2U-Net model separately on DRIVE, STARE, CHASE_DB1, and HRF, using 150 epochs, batch size 16, and binary cross-entropy loss. Checkpoints are saved to handle Kaggle’s session limits.\n",
    "\n",
    "**Output**: For each dataset, prints training progress and displays plots of training/validation loss and accuracy over 150 epochs (or fewer if early stopping triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381f14b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T05:37:55.457789Z",
     "iopub.status.busy": "2025-07-10T05:37:55.457524Z",
     "iopub.status.idle": "2025-07-10T08:18:59.982667Z",
     "shell.execute_reply": "2025-07-10T08:18:59.981836Z"
    },
    "papermill": {
     "duration": 9671.989603,
     "end_time": "2025-07-10T08:19:07.408265",
     "exception": false,
     "start_time": "2025-07-10T05:37:55.418662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "def setup_logging(dataset_name):\n",
    "    logging.basicConfig(\n",
    "        filename=f'training_log_{dataset_name}.txt',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def train_model(model, train_patches, train_labels, val_patches, val_labels, dataset_name, r2unet_func):\n",
    "    \"\"\"Train R2U-Net and save checkpoints and final model.\"\"\"\n",
    "    checkpoint_path = f'r2unet_{dataset_name}_checkpoint.weights.h5'  # Use .weights.h5\n",
    "    final_model_path = f'r2unet_{dataset_name}_final.keras'\n",
    "    dice_checkpoint_path = f'r2unet_{dataset_name}_checkpoint_dice.keras'  # Added for clarity\n",
    "    \n",
    "    # Setup logging\n",
    "    setup_logging(dataset_name)\n",
    "    logging.info(f\"Starting training for {dataset_name}\")\n",
    "    \n",
    "    # Check for existing checkpoint and load if available\n",
    "    initial_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            model.load_weights(checkpoint_path)\n",
    "            logging.info(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "            print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "            initial_epoch = 0  # Adjust based on saved epoch if metadata is available\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load checkpoint {checkpoint_path}: {str(e)}\")\n",
    "            print(f\"Warning: Failed to load checkpoint {checkpoint_path}. Starting from scratch.\")\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        save_weights_only=True\n",
    "    )\n",
    "    dice_checkpoint = DiceCoefficientCallback(\n",
    "        dice_checkpoint_path, \n",
    "        val_patches=val_patches, \n",
    "        val_labels=val_labels, \n",
    "        monitor='val_dice_coefficient', \n",
    "        mode='max'\n",
    "    )\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=15, \n",
    "        restore_best_weights=True, \n",
    "        monitor='val_loss', \n",
    "        mode='min'\n",
    "    )\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-6, \n",
    "        monitor='val_loss', \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # # Data augmentation\n",
    "    # datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True, fill_mode='nearest')\n",
    "    # datagen.fit(train_patches)\n",
    "    \n",
    "    # Compile model with custom loss\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
    "        loss=combined_loss,\n",
    "        metrics=['accuracy', tf.keras.metrics.MeanSquaredError(), dice_coefficient]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    try:\n",
    "        # history = model.fit(\n",
    "        #     datagen.flow(train_patches, train_labels, batch_size=8),\n",
    "        #     validation_data=(val_patches, val_labels),\n",
    "        #     epochs=80,  # Align with provided code\n",
    "        #     initial_epoch=initial_epoch,\n",
    "        #     callbacks=[checkpoint, dice_checkpoint, early_stopping, lr_scheduler]\n",
    "        # )\n",
    "\n",
    "        history = model.fit(\n",
    "            x=train_patches,\n",
    "            y=train_labels,\n",
    "            validation_data=(val_patches, val_labels),\n",
    "            batch_size=8,\n",
    "            epochs=50,\n",
    "            initial_epoch=initial_epoch,\n",
    "            callbacks=[checkpoint, dice_checkpoint, early_stopping, lr_scheduler]\n",
    "        )\n",
    "        \n",
    "        # Log training metrics\n",
    "        for epoch in range(len(history.history['loss'])):\n",
    "            log_message = (\n",
    "                f\"Epoch {epoch + 1}: \"\n",
    "                f\"loss={history.history['loss'][epoch]:.4f}, \"\n",
    "                f\"val_loss={history.history['val_loss'][epoch]:.4f}, \"\n",
    "                f\"val_dice_coefficient={history.history['val_dice_coefficient'][epoch]:.4f}\"\n",
    "            )\n",
    "            logging.info(log_message)\n",
    "        \n",
    "        # Save final model\n",
    "        model.save(final_model_path)\n",
    "        logging.info(f\"Saved final model to {final_model_path}\")\n",
    "        print(f\"Saved final model to {final_model_path}\")\n",
    "\n",
    "        # Evaluate all saved models on validation set\n",
    "        print(f\"\\n\\nEvaluating saved models for {dataset_name} on validation set...\")\n",
    "        saved_models = [\n",
    "           ('Dice Checkpoint', dice_checkpoint_path, True),  # Full model\n",
    "           ('Weights Checkpoint', checkpoint_path, False),   # Weights only\n",
    "           ('Final Model', final_model_path, True)          # Full model\n",
    "        ]\n",
    "        \n",
    "        for model_name, path, is_full_model in saved_models:\n",
    "           try:\n",
    "               if is_full_model:\n",
    "                   eval_model = tf.keras.models.load_model(\n",
    "                       path,\n",
    "                       custom_objects={'combined_loss': combined_loss, 'dice_coefficient': dice_coefficient}\n",
    "                   )\n",
    "                   print(f\"Loaded full model from {path}\")\n",
    "               else:\n",
    "                   eval_model = r2unet_func()  # Recreate model using provided function\n",
    "                   eval_model.compile(\n",
    "                       optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
    "                       loss=combined_loss,\n",
    "                       metrics=['accuracy', tf.keras.metrics.MeanSquaredError(), dice_coefficient]\n",
    "                   )\n",
    "                   eval_model.load_weights(path)\n",
    "                   print(f\"Loaded weights from {path}\")\n",
    "               \n",
    "               # Predict on validation patches\n",
    "               val_pred = eval_model.predict(val_patches, verbose=0)\n",
    "               \n",
    "               # Compute metrics\n",
    "               metrics = compute_metrics(val_labels, val_pred)\n",
    "               print(f\"\\n{dataset_name} {model_name} Validation Metrics:\")\n",
    "               for metric_name, value in metrics.items():\n",
    "                   print(f\"{metric_name}: {value:.4f}\")\n",
    "               \n",
    "               # Clear session to free memory\n",
    "               tf.keras.backend.clear_session()\n",
    "               \n",
    "           except FileNotFoundError:\n",
    "               print(f\"Error: {model_name} file {path} not found. Skipping evaluation.\")\n",
    "           except Exception as e:\n",
    "               print(f\"Error evaluating {model_name} from {path}: {str(e)}\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed for {dataset_name}: {str(e)}\")\n",
    "        print(f\"Error: Training failed for {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Train on each dataset\n",
    "# datasets = {\n",
    "#     'DRIVE': (drive_patches_train, drive_labels_train, drive_patches_val, drive_labels_val),\n",
    "#     'STARE': (stare_patches_train, stare_labels_train, stare_patches_val, stare_labels_val),\n",
    "#     'CHASEDB1': (chase_patches_train, chase_labels_train, chase_patches_val, chase_labels_val),\n",
    "#     'HRF': (hrf_patches_train, hrf_labels_train, hrf_patches_val, hrf_labels_val)\n",
    "# }\n",
    "\n",
    "# Train separate models for each dataset\n",
    "datasets = {\n",
    "    'DRIVE': (drive_patches_train, drive_labels_train, drive_patches_val, drive_labels_val)\n",
    "}\n",
    "\n",
    "models_dict = {}\n",
    "histories = {}\n",
    "for dataset_name, (train_patches, train_labels, val_patches, val_labels) in datasets.items():\n",
    "    print(f\"Training on {dataset_name}...\")\n",
    "    model = r2unet()\n",
    "    # Compile model with custom loss and metrics\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
    "        loss=combined_loss,\n",
    "        metrics=['accuracy', tf.keras.metrics.MeanSquaredError(), dice_coefficient]\n",
    "    )\n",
    "    histories[dataset_name] = train_model(model, train_patches, train_labels, val_patches, val_labels, dataset_name,r2unet_func=r2unet)    \n",
    "    models_dict[dataset_name] = model\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(histories[dataset_name].history['loss'], label='Train Loss')\n",
    "    plt.plot(histories[dataset_name].history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{dataset_name} Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(histories[dataset_name].history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(histories[dataset_name].history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'{dataset_name} Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aab633",
   "metadata": {
    "papermill": {
     "duration": 7.668828,
     "end_time": "2025-07-10T08:19:22.708058",
     "exception": false,
     "start_time": "2025-07-10T08:19:15.039230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Evaluate and Visualize Results\n",
    "\n",
    "We evaluate the trained model on the test sets of each dataset, computing all metrics (AC, SE, SP, F1, DC, JS, AUC) and visualizing sample predictions.\n",
    "\n",
    "**Output**: For each dataset, displays three sample predictions (input, ground truth, prediction) and prints average metrics (AC, SE, SP, F1, DC, JS, AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780bb5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T08:19:38.023207Z",
     "iopub.status.busy": "2025-07-10T08:19:38.022389Z",
     "iopub.status.idle": "2025-07-10T08:19:51.939878Z",
     "shell.execute_reply": "2025-07-10T08:19:51.938770Z"
    },
    "papermill": {
     "duration": 21.640672,
     "end_time": "2025-07-10T08:19:51.941134",
     "exception": false,
     "start_time": "2025-07-10T08:19:30.300462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_images, test_labels, dataset_name):\n",
    "    \"\"\"Evaluate model on test images using patch-based approach and visualize results.\"\"\"\n",
    "    metrics_list = []\n",
    "    for i, (img, label) in enumerate(zip(test_images, test_labels)):\n",
    "        # Ensure input has channel dimension if missing\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[..., np.newaxis]\n",
    "        if len(label.shape) == 2:\n",
    "            label = label[..., np.newaxis]\n",
    "        \n",
    "        # Extract patches\n",
    "        patches, h, w = extract_patches_for_eval(img)\n",
    "        if len(patches) == 0:\n",
    "            print(f\"Warning: No patches extracted for {dataset_name} sample {i+1}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Predict on patches\n",
    "        pred_patches = model.predict(patches, verbose=0)\n",
    "        \n",
    "        # Reconstruct predicted image\n",
    "        pred = reconstruct_image(pred_patches, h, w)\n",
    "        \n",
    "        # Ensure label and pred have compatible shapes\n",
    "        if pred.shape != label.shape:\n",
    "            pred = cv2.resize(pred, (label.shape[1], label.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(label, pred)\n",
    "        metrics_list.append(metrics)\n",
    "        \n",
    "        # Visualize first few samples (full images)\n",
    "        if i < 5:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(img[:, :, 0], cmap='gray')\n",
    "            plt.title('Input Image')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(label[:, :, 0], cmap='gray')\n",
    "            plt.title('Ground Truth')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred[:, :, 0] > 0.5, cmap='gray')\n",
    "            plt.title('Prediction')\n",
    "            plt.suptitle(f'{dataset_name} Test Sample {i+1}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Average metrics\n",
    "    if metrics_list:\n",
    "        avg_metrics = {key: np.mean([m[key] for m in metrics_list]) for key in metrics_list[0]}\n",
    "    else:\n",
    "        avg_metrics = {key: 0.0 for key in ['AC', 'SE', 'SP', 'F1', 'DC', 'JS', 'AUC']}\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "# Evaluate on each dataset using saved models\n",
    "# test_datasets = {\n",
    "#     'DRIVE': (drive_test_images, drive_test_labels),\n",
    "#     'STARE': (stare_test_images, stare_test_labels),\n",
    "#     'CHASEDB1': (chase_test_images, chase_test_labels),\n",
    "#     'HRF': (hrf_test_images, hrf_test_labels)\n",
    "# }\n",
    "\n",
    "# Evaluate on each dataset using saved models\n",
    "test_datasets = {\n",
    "    'DRIVE': (drive_test_images, drive_test_labels)\n",
    "}\n",
    "\n",
    "for dataset_name, (test_images, test_labels) in test_datasets.items():\n",
    "    \n",
    "    print(f\"Evaluating on {dataset_name}...\")\n",
    "    \n",
    "    # Try loading the best model based on val_dice_coefficient\n",
    "    # 1) r2unet_DRIVE_checkpoint.weights.h5: Saved by the ModelCheckpoint callback when the validation loss (val_loss) improves. This represents the model weights with the lowest validation loss during training.\n",
    "    # 2) r2unet_DRIVE_checkpoint_dice.keras: Saved by the DiceCoefficientCallback when the validation Dice Coefficient (val_dice_coefficient) improves. This is a full model file containing the best model based on Dice performance.\n",
    "    # 3) r2unet_DRIVE_final.keras: Saved at the end of training, representing the final model state after 2 epochs, regardless of performance\n",
    "\n",
    "    \n",
    "    dice_checkpoint_path = f'r2unet_{dataset_name}_checkpoint_dice.keras'\n",
    "    weights_checkpoint_path = f'r2unet_{dataset_name}_checkpoint.weights.h5'\n",
    "    final_model_path = f'r2unet_{dataset_name}_final.keras'\n",
    "    \n",
    "    try:\n",
    "        model = tf.keras.models.load_model(\n",
    "            dice_checkpoint_path,\n",
    "            custom_objects={'combined_loss': combined_loss, 'dice_coefficient': dice_coefficient}\n",
    "        )\n",
    "        print(f\"Loaded best model from {dice_checkpoint_path}\")\n",
    "        metrics = evaluate_model(model, test_images, test_labels, dataset_name)\n",
    "        print(f\"{dataset_name} Metrics: {metrics}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file {dice_checkpoint_path} not found. Trying weights checkpoint...\")\n",
    "        try:\n",
    "            model = r2unet()  # Initialize model\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
    "                loss=combined_loss,\n",
    "                metrics=['accuracy', tf.keras.metrics.MeanSquaredError(), dice_coefficient]\n",
    "            )\n",
    "            model.load_weights(weights_checkpoint_path)\n",
    "            print(f\"Loaded weights from {weights_checkpoint_path}\")\n",
    "            metrics = evaluate_model(model, test_images, test_labels, dataset_name)\n",
    "            print(f\"{dataset_name} Metrics: {metrics}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Weights file {weights_checkpoint_path} not found. Trying final model...\")\n",
    "            try:\n",
    "                model = tf.keras.models.load_model(\n",
    "                    final_model_path,\n",
    "                    custom_objects={'combined_loss': combined_loss, 'dice_coefficient': dice_coefficient}\n",
    "                )\n",
    "                print(f\"Loaded final model from {final_model_path}\")\n",
    "                metrics = evaluate_model(model, test_images, test_labels, dataset_name)\n",
    "                print(f\"{dataset_name} Metrics: {metrics}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: Final model file {final_model_path} not found. Please train and save the model first.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7816660,
     "sourceId": 12395751,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9789.154054,
   "end_time": "2025-07-10T08:20:02.465438",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-10T05:36:53.311384",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
